{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T08:31:57.051982Z",
     "start_time": "2021-04-22T08:31:57.035027Z"
    }
   },
   "outputs": [],
   "source": [
    "# Version Requirements\n",
    "# Windows 10\n",
    "# MongoDB 4.4.3\n",
    "# Python: 3.8.8\n",
    "# jupyter-notebook: 6.3.0\n",
    "# numpy: 1.19.5\n",
    "# yfinance: 0.1.59\n",
    "# tweepy: 3.10.0\n",
    "# tensorflow: 2.4.1\n",
    "# seaborn: 0.11.1\n",
    "# scikit-learn: 0.24.1\n",
    "# pymongo: 3.11.3\n",
    "# pandas: 1.2.4\n",
    "# nltk: 3.6.1\n",
    "# mglearn: 0.1.9\n",
    "# matplotlib: 3.3.4\n",
    "# emoji: 1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T15:48:17.686728Z",
     "start_time": "2021-04-23T15:48:17.669772Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growth SET\n",
      "GPU ACTIVE\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import time, copy \n",
    "import pandas as pd\n",
    "import yfinance as yf \n",
    "import seaborn as sns\n",
    "from joblib import dump,load\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime, timedelta,date\n",
    "import tweepy, re, io, csv, os, pymongo, numpy as np\n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "\n",
    "#sentiment\n",
    "import nltk\n",
    "import mglearn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#forecasting component \n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"Growth SET\")\n",
    "except:\n",
    "    print(\"Invalid device OR cannot modify virtual devices once initialized.\")\n",
    "    pass\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "import kerastuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer,SnowballStemmer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import emoji\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 5]\n",
    "print( \"GPU ACTIVE\" if len(tf.config.list_physical_devices('GPU'))>0 else \"GPU NOT AVAILABLE\")\n",
    "# print(tf.keras.mixed_precision.global_policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T15:48:17.922098Z",
     "start_time": "2021-04-23T15:48:17.909132Z"
    }
   },
   "outputs": [],
   "source": [
    "# mydb[\"ProcessedTweets\"].update_many({}, {\"$unset\": {\"poster\":\"\"}},True);\n",
    "# unprocessed.update_many({}, {\"$unset\": {\"poster\":\"\"}},True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:51:51.523153Z",
     "start_time": "2021-04-23T12:51:51.509191Z"
    }
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler('9tpKa7Rs0azYhUUZQ9t13zZbR', 'NJvdMLo901KiycSjlz1NlshQaM8iJmRLfHCXZhZXgUzwy2cl4y')\n",
    "auth.set_access_token(\"1070877312604954625-91M4PyNC72QYws3lIm90mj5yIjElVU\", \"EpARpeIQECJDEWRe2WfWg239FQaHoWZa4xFreDMTm84Zf\")\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:51:52.504551Z",
     "start_time": "2021-04-23T12:51:51.540109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'limit': 15, 'remaining': 15, 'reset': 1619183211}\n",
      "{'limit': 900, 'remaining': 900, 'reset': 1619183211}\n",
      "sentimentDB exists.\n",
      "Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'sentimentDB')\n"
     ]
    }
   ],
   "source": [
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "data = api.rate_limit_status()\n",
    "print (data['resources']['statuses']['/statuses/home_timeline'])\n",
    "print (data['resources']['users']['/users/lookup'])\n",
    "mydb = None\n",
    "if \"sentimentDB\" in myclient.list_database_names():\n",
    "    print(\"sentimentDB exists.\")\n",
    "    mydb = myclient[\"sentimentDB\"]\n",
    "else:\n",
    "    print(\"sentimentDB Didn't Exist and Has Been Created\\n\")\n",
    "    mydb = myclient[\"sentimentDB\"]\n",
    "print(mydb)\n",
    "processed = mydb[\"Processed\"]\n",
    "sentiment140 = mydb[\"sentiment140\"]\n",
    "unprocessed = mydb[\"Unprocessed\"]\n",
    "unprocessedArchive = mydb[\"Unprocessed Archive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:51:52.534471Z",
     "start_time": "2021-04-23T12:51:52.520509Z"
    }
   },
   "outputs": [],
   "source": [
    "def printTweet(tweet): \n",
    "    print(\"T: \" + str(tweet.created_at)) \n",
    "    if hasattr(tweet, \"retweeted_status\"):  # Check if Retweet\n",
    "        try:\n",
    "            print(tweet.retweeted_status.extended_tweet[\"full_text\"])\n",
    "        except AttributeError:\n",
    "            print(tweet.retweeted_status.text)\n",
    "    else:\n",
    "        try:\n",
    "            print(tweet.extended_tweet[\"full_text\"])\n",
    "        except AttributeError:\n",
    "            print(tweet.full_text)\n",
    "    print(\"Retweet Count:\\t{0}\".format(int(tweet.retweet_count)))\n",
    "    print(\"Poster screen name: \" + tweet.user.screen_name) \n",
    "    print(\"Retweet_count: \" + str(tweet.retweet_count)) \n",
    "    print(\"Favorite_count: \" + str(tweet.favorite_count)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:51:52.565388Z",
     "start_time": "2021-04-23T12:51:52.550428Z"
    }
   },
   "outputs": [],
   "source": [
    "def insertDFToDB(column, dataframe):\n",
    "    mydb[column].drop()\n",
    "    return mydb[column].insert_many(dataframe.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:51:52.596305Z",
     "start_time": "2021-04-23T12:51:52.581346Z"
    }
   },
   "outputs": [],
   "source": [
    "def insertToDB(tweet, stockInfo,query):\n",
    "    t=datetime.utcnow()  \n",
    "    tweetText=\"String Error Did Not Find String.\"\n",
    "    sensitive=None\n",
    "    if hasattr(tweet, \"retweeted_status\"):  # Check if Retweet\n",
    "        try:\n",
    "            tweetText=(tweet.retweeted_status.extended_tweet[\"full_text\"])\n",
    "        except AttributeError:\n",
    "            tweetText=(tweet.retweeted_status.text)\n",
    "    else:\n",
    "        try:\n",
    "            tweetText=(tweet.extended_tweet[\"full_text\"])\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                tweetText=(tweet.full_text)\n",
    "            except AttributeError:\n",
    "                tweetText=(tweet.text)\n",
    "    try:\n",
    "        sensitive=str(tweet.possibly_sensitive)\n",
    "    except: \n",
    "        tweet.possibly_sensitive=\"Unknown\"\n",
    "    \n",
    "    item= { \"dateRetrieved\"    :t,\n",
    "            \"dateCreated\"      :t,\n",
    "            \"interval(S)\"      :(t-tweet.created_at).total_seconds(),\n",
    "            \"stockInfo\"        :stockInfo,\n",
    "            \"query\"            :query,\n",
    "            \"_id\"              :str(tweet.id_str),\n",
    "#             \"poster\"           :str(tweet.user.screen_name),\n",
    "            \"posterID\"         :str(tweet.user.id_str),\n",
    "            \"posterVerified\"   :tweet.user.verified,\n",
    "            \"followers\"        :int(tweet.user.followers_count),\n",
    "            \"friends\"          :int(tweet.user.friends_count),\n",
    "            \"likes\"            :int(tweet.favorite_count),\n",
    "            \"retweets\"         :int(tweet.retweet_count),\n",
    "#             \"quotes\"         :int(tweet.quote_count),\n",
    "            \"tweetText\"        :tweetText,\n",
    "            \"sensitive\"        :sensitive,\n",
    "            \"isRetweet\"        :False,\n",
    "            \"isQuote\"          :False\n",
    "          }\n",
    "    try:\n",
    "        unprocessed.insert_one(item)\n",
    "        return False\n",
    "    except pymongo.errors.DuplicateKeyError:\n",
    "#         print(\"duplicate entry found\")\n",
    "        unprocessed.replace_one({'_id': tweet.id_str}, item, upsert=True)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:51:52.628220Z",
     "start_time": "2021-04-23T12:51:52.613261Z"
    }
   },
   "outputs": [],
   "source": [
    "def SearchStore(query,stockinfo ,sinceDate=str(datetime.utcnow() - timedelta(days=30))[:10],untilDate=str(datetime.utcnow())[:10],sID=None, rtmin=2,count=10000):\n",
    "    counter,updateC,newEntries=0,0,0\n",
    "    for status in tweepy.Cursor(api.search, include_rts =False, include_replies=False, \n",
    "                                since=sinceDate, until=untilDate,tweet_mode=\"extended\"\n",
    "                                ,q=query+\" exclude:retweets AND exclude:replies AND min_retweets:\"+str(rtmin), lang=\"en\").items():\n",
    "        if(insertToDB(status,stockinfo,query)):updateC+=1            \n",
    "        else:newEntries+=1\n",
    "        counter+=1\n",
    "        if counter>=count: break\n",
    "    if counter!=count: print(\"End Reached\")\n",
    "    print(stockinfo+\"\\tEntries: {0}\\tUpdated: {1}\\tNew: {2}\".format(counter,updateC,newEntries))    \n",
    "    \n",
    "def SweepStore(query,stockinfo ,sID=None, rtmin=2,count=10000):\n",
    "    print(query,\": \",count)\n",
    "    for i in range (31,0,-1):\n",
    "        dayt0=datetime.utcnow().date()- timedelta(days=i)\n",
    "        dayt1=dayt0+timedelta(days=1)\n",
    "        print(dayt0,\"\\t\",dayt1)\n",
    "        SearchStore(query,stockinfo ,sinceDate=dayt0,untilDate=dayt1,count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:51:52.660135Z",
     "start_time": "2021-04-23T12:51:52.645175Z"
    }
   },
   "outputs": [],
   "source": [
    "def ArchiveStore(q, sinceDate, untilDate,stockInfo=\"\", limit=100):\n",
    "    t=datetime.utcnow()\n",
    "    count=0\n",
    "    for tweet in tweepy.Cursor(api.search_full_archive,environment_name ='Sentiment', query =q+\" lang:en\",\n",
    "                                fromDate=sinceDate, toDate=untilDate).items():\n",
    "        tweetText=\"String Error Did Not Find String.\"\n",
    "        sensitive=None\n",
    "        if hasattr(tweet, \"retweeted_status\"):\n",
    "            try:\n",
    "                tweetText=(tweet.retweeted_status.extended_tweet[\"full_text\"])\n",
    "            except AttributeError:\n",
    "                tweetText=(tweet.retweeted_status.text)\n",
    "        else:\n",
    "            try:\n",
    "                tweetText=(tweet.extended_tweet[\"full_text\"])\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    tweetText=(tweet.full_text)\n",
    "                except AttributeError:\n",
    "                    tweetText=(tweet.text)\n",
    "        try:\n",
    "            sensitive=str(tweet.possibly_sensitive)\n",
    "        except: \n",
    "            tweet.possibly_sensitive=\"Unknown\"\n",
    "        count+=1\n",
    "        item= {     \"dateRetrieved\"    :t,\n",
    "                    \"dateCreated\"      :tweet.created_at,\n",
    "                    \"interval(S)\"      :(t-tweet.created_at).total_seconds(),\n",
    "                    \"stockInfo\"        :stockInfo,\n",
    "                    \"query\"            :q,\n",
    "                    \"_id\"              :str(tweet.id_str),\n",
    "                    \"poster\"           :str(tweet.user.screen_name),\n",
    "#                     \"posterID\"         :str(tweet.user.id_str),\n",
    "                    \"posterVerified\"   :tweet.user.verified,\n",
    "                    \"followers\"        :int(tweet.user.followers_count),\n",
    "                    \"friends\"          :int(tweet.user.friends_count),\n",
    "                    \"likes\"            :int(tweet.favorite_count),\n",
    "                    \"retweets\"         :int(tweet.retweet_count),\n",
    "                    \"quotes\"           :int(tweet.quote_count),\n",
    "                    \"tweetText\"        :tweetText,\n",
    "                    \"sensitive\"        :sensitive,\n",
    "                    \"isRetweet\"        :tweet.retweeted,\n",
    "                    \"isQuote\"          :tweet.is_quote_status\n",
    "                  }\n",
    "        try:\n",
    "            unprocessedArchive.insert_one(item)\n",
    "        except pymongo.errors.DuplicateKeyError:\n",
    "            print(\"duplicate entry found\")\n",
    "            unprocessedArchive.replace_one({'_id': tweetID}, item, upsert=True)\n",
    "        if(count>=limit):\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:51:52.692050Z",
     "start_time": "2021-04-23T12:51:52.677090Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadStockData(stock,tick=False,save=True):\n",
    "    stocktick = yf.Ticker(stock)\n",
    "    stockhistory=stocktick.history(period=\"max\")\n",
    "    stockhistory[\"Date\"]=stockhistory.index\n",
    "    mydb[stock].drop()\n",
    "    mydb[stock].insert_many(stockhistory.to_dict(orient='records'))\n",
    "    if tick: return stockhistory,stocktick\n",
    "    return stockhistory\n",
    "\n",
    "def getQueryFromMongoDB(tablename,query={},fields=None):\n",
    "    table=mydb[tablename]\n",
    "    values=[]\n",
    "    if fields is not None:\n",
    "        for entry in table.find(query,fields):\n",
    "            values.append(entry)\n",
    "    else :\n",
    "        for entry in table.find(query):\n",
    "            values.append(entry)\n",
    "    return pd.DataFrame(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:51:52.723964Z",
     "start_time": "2021-04-23T12:51:52.709005Z"
    }
   },
   "outputs": [],
   "source": [
    "#optimizations \n",
    "# lemmatizer = WordNetLemmatizer() \n",
    "lemmatizer = SnowballStemmer(\"english\") \n",
    "# lemmatizer = PorterStemmer()\n",
    "# stemm=lemmatizer.lemmatize\n",
    "stemm=lemmatizer.stem\n",
    "stopwordsetNP=[re.sub(r\"'\",'',w) for w in stopwords.words('english')]\n",
    "\n",
    "#String preprocessing method\n",
    "def preprocess(text,POS=False,chunk=False,demojize=False):\n",
    "        if demojize: text=emoji.demojize(text, use_aliases=True, delimiters=(' ', ' '))\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"((((https?)?:\\/\\/)(www\\.)?|www\\.))([a-z0-9.]+)(\\.[a-z]{2,4})(\\.[a-z]{1,2})?([^?\\s]+(\\?((\\w+)(=[^&\\s]+)?&?)+)?)?\",' LINKTAG ',text)    \n",
    "        text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+\\.[\\w\\.\\_]+',' EMAILTAG ', text)\n",
    "        text = re.sub(r'(@[a-z0-9_]+)',' USERNAMETAG ', text) #1\n",
    "        text = re.sub(r\"\\#[a-z0-9_-]+\", \" HASHTAG \", text)\n",
    "        text = re.sub(r\"[']+\",'', text)\n",
    "        text = re.sub(r\"[^ a-zA-Z]+\", \" \", text)#1\n",
    "        text = re.sub(r\"\\b(.)\\1{1,}\", r\" \\1\", text)\n",
    "        text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "        text = re.sub(r\" \\w \", \" \", text)\n",
    "        text = wpt.tokenize(text)\n",
    "        text=[stemm(w) for w in text]\n",
    "        if POS:\n",
    "            text = nltk.pos_tag(text)\n",
    "            if chunk: text = nltk.ne_chunk(text, binary=True)\n",
    "            tokens = []\n",
    "            for word in text:\n",
    "                if(isinstance(word[0],str)): tokens.append(word[0] + \"N\" + word[1])\n",
    "                else:\n",
    "                    temp=\"\"\n",
    "                    for w in word: temp+=\"CHUNK\"+(w[0] + \"N\" + w[1])\n",
    "                    tokens.append(temp)\n",
    "        text = TreebankWordDetokenizer().detokenize(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:51:53.460006Z",
     "start_time": "2021-04-23T12:51:53.449036Z"
    }
   },
   "outputs": [],
   "source": [
    "# file= \"project/a/TrainiingNOemoticon.csv\"\n",
    "# df = pd.read_csv(file, header=None)\n",
    "# for index,row in df.iterrows():\n",
    "#     x,t_Id,date,c,user,text=row\n",
    "#     item= {     \n",
    "#             \"SentimentScore\"   :x,\n",
    "#             \"_id\"              :t_Id,\n",
    "#             \"date\"             :date,\n",
    "#             \"poster\"           :user,\n",
    "#             \"tweetText\"        :text\n",
    "#           }\n",
    "#     try:\n",
    "#         sentiment140.insert_one(item)\n",
    "#     except pymongo.errors.DuplicateKeyError:\n",
    "#         print(\"duplicate entry found\")\n",
    "#         continue\n",
    "# file= \"project/archive/Stocks/tsla.us.txt\"\n",
    "# sentiment140 = mydb[\"stock inf\"]\n",
    "# df = pd.read_csv(file, header=None,encoding='latin-1')\n",
    "# for index,row in df.iterrows():\n",
    "#     x,t_Id,date,c,user,text=row\n",
    "#     item= {     \n",
    "#             \"Date\":,\n",
    "#             \"Open\":,\n",
    "#             \"High\":,\n",
    "#             \"Low\":,\n",
    "#             \"Close\":,\n",
    "#             \"Volume\":,\n",
    "#             \"OpenInt\":\n",
    "#           }\n",
    "#     try:\n",
    "#         sentiment140.insert_one(item)\n",
    "#     except pymongo.errors.DuplicateKeyError:\n",
    "#         print(\"duplicate entry found\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T08:32:10.442351Z",
     "start_time": "2021-04-22T08:32:10.425396Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model,**kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "        self.supports_masking = True\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],d_model=d_model)\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "class ScalerMult(Layer):\n",
    "    def __init__(self, d_model,**kwargs):\n",
    "        super(ScalerMult, self).__init__(**kwargs)\n",
    "        self.mult = tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "        self.supports_masking = True\n",
    "    def call(self, inputs):\n",
    "        return inputs * self.mult   \n",
    "\n",
    "def Encoder(dim,encoders,hidden,P,key_dim,dropout=0.2,axis=-1,epsilon=1e-6,activation=\"elu\"):\n",
    "    temp=[None]*encoders\n",
    "    for e in range(encoders):\n",
    "        AT= MultiHeadAttentionMaskProp(dim, key_dim,dropout)(P)\n",
    "        AD1 = Add()([AT, P])\n",
    "        NORM1 = LayerNormalization(epsilon=epsilon, axis=axis)(AD1)\n",
    "        FF = Dense(hidden, activation)(NORM1)\n",
    "        FF = Dropout(dropout)(FF)\n",
    "        FF = Dense(dim, activation)(FF)\n",
    "        AD2 = Add()([FF, NORM1])\n",
    "        temp[e]=LayerNormalization(epsilon=epsilon, axis=axis)(AD2)\n",
    "    if encoders==1: return temp[0]\n",
    "    return concatenate(temp) \n",
    "\n",
    "class MultiHeadAttentionMaskProp(layers.Layer):\n",
    "    def __init__(self ,dim, key_dim,dropout=0, **kwargs):\n",
    "        super(MultiHeadAttentionMaskProp,self).__init__(**kwargs)\n",
    "        self.MultiHeadAttention = MultiHeadAttention(num_heads=dim, key_dim=key_dim,dropout=dropout)\n",
    "        self.supports_masking = True\n",
    "    def call(self, inputs, mask=None): \n",
    "        if mask is None: return self.MultiHeadAttention(inputs,inputs,inputs)  \n",
    "        else :return self.MultiHeadAttention(inputs,inputs,inputs,mask[:, tf.newaxis, tf.newaxis, :])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentimentComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:55:23.661073Z",
     "start_time": "2021-04-22T11:55:23.633147Z"
    }
   },
   "outputs": [],
   "source": [
    "def findMissingInterval(data):\n",
    "    missingIntervals=[]\n",
    "    start=None\n",
    "    end=None\n",
    "    daysBetween=0\n",
    "    ss=pd.date_range(data.index.min(), data.index.max())\n",
    "    for i in range(len(ss)):\n",
    "        if((ss[i].strftime('%Y-%m-%d')==data.index).sum()>=1):\n",
    "            if start!=None: \n",
    "                missingIntervals.append([start,ss[i],daysBetween])\n",
    "                start=None\n",
    "                daysBetween=0\n",
    "        elif((ss[i].strftime('%Y-%m-%d')==data.index).sum()==0):\n",
    "            if(start==None): start=ss[i-1]\n",
    "            daysBetween+=1\n",
    "    return pd.DataFrame(data=missingIntervals,columns=['From', 'To', 'DaysGap'])\n",
    "def checkNonEmptyRuns(data):\n",
    "    runs=[]\n",
    "    start=None\n",
    "    end=None\n",
    "    daysBetween=0\n",
    "    total=0\n",
    "    ss=pd.date_range(data.index.min(), data.index.max())\n",
    "    for i in range(len(ss)):\n",
    "        if((ss[i].strftime('%Y-%m-%d')==data.index).sum()>=1):\n",
    "            if(start==None): \n",
    "                start=ss[i]\n",
    "            daysBetween+=1\n",
    "        elif((ss[i].strftime('%Y-%m-%d')==data.index).sum()==0):\n",
    "            if start!=None: \n",
    "                runs.append([start,ss[i-1],daysBetween])\n",
    "                start=None\n",
    "                total+=daysBetween\n",
    "                daysBetween=0\n",
    "    if start!=None: \n",
    "        runs.append([start,ss[-1],daysBetween])\n",
    "        total+=daysBetween\n",
    "    return pd.DataFrame(data=runs,columns=['From', 'To', 'Days'])\n",
    "def dataSquencer(data,sequenceLength=12,endLength=1):\n",
    "    xSequences=[]\n",
    "    ySequences=[]\n",
    "    sequenceLength+=endLength\n",
    "    entryList=[]\n",
    "    ss=pd.date_range(data.index.min(), data.index.max())\n",
    "    daysBetween=0\n",
    "    for i in range(len(ss)):\n",
    "        sss=ss[i].strftime('%Y-%m-%d')\n",
    "        if((sss==data.index).sum()>=1):\n",
    "            entryList.append(data.loc[sss].values)\n",
    "            daysBetween+=1\n",
    "        elif((sss==data.index).sum()==0):\n",
    "            if(sequenceLength<=daysBetween):\n",
    "                xxs,yys=sequenceBreak(entryList,sequenceLength-endLength,endLength)\n",
    "                xSequences.extend(xxs)\n",
    "                ySequences.extend(yys)\n",
    "            entryList=[]\n",
    "            daysBetween=0\n",
    "    if(sequenceLength<=daysBetween):\n",
    "        xxs,yys=sequenceBreak(entryList,sequenceLength-endLength,endLength)\n",
    "        xSequences.extend(xxs)\n",
    "        ySequences.extend(yys)\n",
    "    return xSequences,ySequences\n",
    "def sequenceBreak(data,sequenceLength=12,endLength=1):\n",
    "    size=len(data)-sequenceLength-endLength+1\n",
    "    brokenSequenceXs,brokenSequenceYs=[None]*size,[None]*size\n",
    "    for i in range(size):\n",
    "        brokenSequenceXs[i]=data[i:i+sequenceLength]\n",
    "        brokenSequenceYs[i]=data[sequenceLength+i:i+sequenceLength+endLength]\n",
    "    return brokenSequenceXs, brokenSequenceYs\n",
    "\n",
    "# def printResults(ytrue,pred):\n",
    "#     MeanDirectionAccuracy=np.average(np.sign(ytrue-pred))\n",
    "#     RMSE=mean_squared_error(ytrue.flatten(), pred.flatten(),squared=False)\n",
    "#     print(\"MSE    :\",RMSE**2,#Mean Squared Error (MSE)\n",
    "#           \"\\nRMSE   :\",RMSE,#Root Mean Square Error\n",
    "#           \"\\nNMSE   :\",RMSE/(ytrue.max()-ytrue.min()),#Normalise Mean Square Error(/max-min)\n",
    "#           \"\\nDS     :\",MeanDirectionAccuracy,#MeanDirectionAccuracy\n",
    "#           \"\\nWDS    :\",MeanDirectionAccuracy/(ytrue.max()-ytrue.min()),#Weighted Directional Symmetry\n",
    "#           \"\\nSamples:\",len(ytrue))#Test Samples\n",
    "#     return\n",
    "def getResults(ytrue,pred,printR=True):\n",
    "    EstimationDirection=np.average(np.sign(pred-ytrue))\n",
    "    RMSE=mean_squared_error(ytrue.flatten(), pred.flatten(),squared=False)\n",
    "    R=[RMSE**2,RMSE,RMSE/(ytrue.max()-ytrue.min()),EstimationDirection,EstimationDirection/(ytrue.max()-ytrue.min()),len(ytrue)]\n",
    "    if printR:\n",
    "        print(\"MSE:                       \",R[0],#Mean Squared Error (MSE)\n",
    "              \"\\nRMSE:                      \",R[1],#Root Mean Square Error\n",
    "              \"\\nNMSE:                      \",R[2],#Normalise Mean Square Error(/max-min)\n",
    "              \"\\nEstimation Symmetry:       \",R[3],#Estimation symmetry from actual Value\n",
    "              \"\\nNormed Estimation Symmetry:\",R[4],#Normalized Estimation symmetry from actual Value\n",
    "              \"\\nSamples:                   \",R[5])#Test Samples\n",
    "    return R\n",
    "    \n",
    "    \n",
    "def plotCompare(x,y,labels=None,size=(20,10)):\n",
    "    if labels is None:\n",
    "        labels=[\"X,Y\"]\n",
    "    plt.plot(x, label=labels[0])\n",
    "    plt.plot(y, label=labels[1])\n",
    "    plt.legend()\n",
    "    plt.show()        \n",
    "# def makeTrainValidationTestSplit(dataX,dataY=None,splits=[0.6,0.2],randomize=False,reshapeX=None,reshapeY=None,state=0):\n",
    "#     if randomize: dataX,dataY=shuffle(dataX,dataY, random_state=state)\n",
    "#     if reshapeX is not None: dataX=dataX.reshape(reshapeX)\n",
    "#     if reshapeY is not None: dataY=dataY.reshape(reshapeY)\n",
    "#     TLim=round ((dataX.shape[0])*splits[0])\n",
    "#     VLim=round (dataX.shape[0]*splits[1])\n",
    "#     trainX,valX,testX=dataX[:TLim],dataX[TLim:VLim+TLim],dataX[VLim+TLim:]\n",
    "#     if dataY is not None:\n",
    "#         trainY,valY,testY=dataY[:TLim],dataY[TLim:VLim+TLim],dataY[VLim+TLim:]\n",
    "#         return trainX,valX,testX,trainY,valY,testY\n",
    "#     return trainX,valX,testX\n",
    "def createTimeSeriesData(data,inputColumns,shifts=5,outputColumns=None,outputlengthTimeSteps=1):\n",
    "    temp=data[inputColumns]\n",
    "    for i in range(1,shifts): \n",
    "        temp=pd.merge(left=data[inputColumns].shift(i),left_index=True, right=temp,right_index=True)\n",
    "    temp=temp.dropna()\n",
    "    if outputColumns is not None: \n",
    "        return temp[:-outputlengthTimeSteps],createTimeSeriesData(data,outputColumns,shifts=outputlengthTimeSteps)[shifts:]\n",
    "    return temp\n",
    "def StepForwardPredictions(vx,model,steps):\n",
    "    vx=vx.copy()\n",
    "    for ii in range(steps): vx=np.insert(vx, period+ii, model.predict(vx[:,ii:])[:,:,0], axis=1)\n",
    "    return vx[:,period:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
