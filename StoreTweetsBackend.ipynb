{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growth SET\n",
      "GPU ACTIVE\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import time, copy \n",
    "import pandas as pd\n",
    "import yfinance as yf \n",
    "import seaborn as sns\n",
    "from joblib import dump,load\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime, timedelta,date\n",
    "import tweepy, re, io, csv, os, pymongo, numpy as np\n",
    "%matplotlib inline\n",
    "#sentiment\n",
    "import nltk\n",
    "import mglearn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#forecasting component \n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"Growth SET\")\n",
    "except:\n",
    "    print(\"Invalid device OR cannot modify virtual devices once initialized.\")\n",
    "    pass\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.activations import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 5]\n",
    "print( \"GPU ACTIVE\" if len(tf.config.list_physical_devices('GPU'))>0 else \"GPU NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler('9tpKa7Rs0azYhUUZQ9t13zZbR', 'NJvdMLo901KiycSjlz1NlshQaM8iJmRLfHCXZhZXgUzwy2cl4y')\n",
    "auth.set_access_token(\"1070877312604954625-91M4PyNC72QYws3lIm90mj5yIjElVU\", \"EpARpeIQECJDEWRe2WfWg239FQaHoWZa4xFreDMTm84Zf\")\n",
    "# auth = tweepy.OAuthHandler('qu53XjVh2PxvXPTeYfCaW5Ujb', 'PYtXz1VB0B9AhDusAzreRnVZso9hfeSUyA0jAhpQqxDtpziM3e')\n",
    "# auth.set_access_token(\"1070877312604954625-h2OYl24cl3a01CYM2QHuZIG2ykEvTy\", \"1ZoLMdm663zDbuNGDdmx1OhEJKgjVYI7WOOmwnfCITbFV\")\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'limit': 15, 'remaining': 15, 'reset': 1616024585}\n",
      "{'limit': 900, 'remaining': 900, 'reset': 1616024585}\n",
      "sentimentDB exists.\n",
      "Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'sentimentDB')\n"
     ]
    }
   ],
   "source": [
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "DBlist=myclient.list_database_names()\n",
    "data = api.rate_limit_status()\n",
    "print (data['resources']['statuses']['/statuses/home_timeline'])\n",
    "print (data['resources']['users']['/users/lookup'])\n",
    "mydb = None\n",
    "if \"sentimentDB\" in DBlist:\n",
    "    print(\"sentimentDB exists.\")\n",
    "    mydb = myclient[\"sentimentDB\"]\n",
    "else:\n",
    "    print(\"sentimentDB Didn't Exist and Has Been Created\\n\")\n",
    "    mydb = myclient[\"sentimentDB\"]\n",
    "print(mydb)\n",
    "processed = mydb[\"Processed\"]\n",
    "sentiment140 = mydb[\"sentiment140\"]\n",
    "unprocessed = mydb[\"Unprocessed\"]\n",
    "unprocessedArchive = mydb[\"Unprocessed Archive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTweet(tweet): \n",
    "    print(\"T: \" + str(tweet.created_at)) \n",
    "    # print(\"The id is : \" + str(tweet.id)) \n",
    "    # print(\"The id_str is : \" + tweet.id_str) \n",
    "#   # print(tweet.extended_tweet)\n",
    "    if hasattr(tweet, \"retweeted_status\"):  # Check if Retweet\n",
    "        try:\n",
    "            print(tweet.retweeted_status.extended_tweet[\"full_text\"])\n",
    "        except AttributeError:\n",
    "            print(tweet.retweeted_status.text)\n",
    "    else:\n",
    "        try:\n",
    "            print(tweet.extended_tweet[\"full_text\"])\n",
    "        except AttributeError:\n",
    "            print(tweet.full_text)\n",
    "    print(\"Retweet Count:\\t{0}\".format(int(tweet.retweet_count)))\n",
    "    # print(\"The entitities are : \" + str(tweet.entities)) \n",
    "    # print(\"The source is : \" + tweet.source) \n",
    "    # print(\"The source_url is : \" + tweet.source_url)   \n",
    "    # print(\"The in_reply_to_status_id is : \" + str(tweet.in_reply_to_status_id)) \n",
    "    # print(\"The in_reply_to_status_id_str is : \" + str(tweet.in_reply_to_status_id_str)) \n",
    "    # print(\"The in_reply_to_user_id is : \" + str(tweet.in_reply_to_user_id)) \n",
    "    # print(\"The in_reply_to_user_id_str is : \" + str(tweet.in_reply_to_user_id_str)) \n",
    "    # print(\"The in_reply_to_screen_name is : \" + str(tweet.in_reply_to_screen_name)) \n",
    "    print(\"The poster's screen name is : \" + tweet.user.screen_name) \n",
    "    print(\"The poster's name is : \" + tweet.user.name) \n",
    "    # print(\"The geo is : \" + str(tweet.geo)) \n",
    "    # print(\"The coordinates are : \" + str(tweet.coordinates)) \n",
    "    # print(\"The place is : \" + str(tweet.place)) \n",
    "    print(\"The contributors are : \" + str(tweet.contributors)) \n",
    "    print(\"The is_quote_status is : \" + str(tweet.is_quote_status)) \n",
    "    print(\"The retweet_count is : \" + str(tweet.retweet_count)) \n",
    "    print(\"The favorite_count is : \" + str(tweet.favorite_count)) \n",
    "    # print(\"Has the authenticated user favourited the status? : \" + str(tweet.favorited)) \n",
    "    # print(\"Has the authenticated user retweeted the status? \" + str(tweet.retweeted)) \n",
    "    # try:\n",
    "    #     print(\"Is the status possibly_sensitive? : \" + str(tweet.possibly_sensitive)) \n",
    "    # except: \n",
    "    #     tweet.possibly_sensitive=\"Unknown\"\n",
    "    #     print(\"Is the status possibly_sensitive? : \" + str(tweet.possibly_sensitive)) \n",
    "    # print(\"The lang is : \" + tweet.lang) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insterDFToDB(column, dataframe):\n",
    "    return mydb[column].insert_many(dataframe.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertToDB(tweet, stockInfo,query):\n",
    "    t=datetime.utcnow()  \n",
    "    tweetText=\"String Error Did Not Find String.\"\n",
    "    sensitive=None\n",
    "    if hasattr(tweet, \"retweeted_status\"):  # Check if Retweet\n",
    "        try:\n",
    "            tweetText=(tweet.retweeted_status.extended_tweet[\"full_text\"])\n",
    "        except AttributeError:\n",
    "            tweetText=(tweet.retweeted_status.text)\n",
    "    else:\n",
    "        try:\n",
    "            tweetText=(tweet.extended_tweet[\"full_text\"])\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                tweetText=(tweet.full_text)\n",
    "            except AttributeError:\n",
    "                tweetText=(tweet.text)\n",
    "    try:\n",
    "        sensitive=str(tweet.possibly_sensitive)\n",
    "    except: \n",
    "        tweet.possibly_sensitive=\"Unknown\"\n",
    "    \n",
    "    item= { \"dateRetrieved\"    :t,\n",
    "            \"dateCreated\"      :t,\n",
    "            \"interval(S)\"      :(t-tweet.created_at).total_seconds(),\n",
    "            \"stockInfo\"        :stockInfo,\n",
    "            \"query\"            :query,\n",
    "            \"_id\"              :str(tweet.id_str),\n",
    "            \"poster\"           :str(tweet.user.screen_name),\n",
    "            \"posterID\"         :str(tweet.user.id_str),\n",
    "            \"posterVerified\"   :tweet.user.verified,\n",
    "            \"followers\"        :int(tweet.user.followers_count),\n",
    "            \"friends\"          :int(tweet.user.friends_count),\n",
    "            \"likes\"            :int(tweet.favorite_count),\n",
    "            \"retweets\"         :int(tweet.retweet_count),\n",
    "#             \"quotes\"         :int(tweet.quote_count),\n",
    "            \"tweetText\"        :tweetText,\n",
    "            \"sensitive\"        :sensitive,\n",
    "            \"isRetweet\"        :False,\n",
    "            \"isQuote\"          :False\n",
    "          }\n",
    "    try:\n",
    "        unprocessed.insert_one(item)\n",
    "        return False\n",
    "    except pymongo.errors.DuplicateKeyError:\n",
    "#         print(\"duplicate entry found\")\n",
    "        unprocessed.replace_one({'_id': tweet.id_str}, item, upsert=True)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchStore(query,stockinfo ,sinceDate=str(datetime.utcnow() - timedelta(days=30))[:10],\n",
    "                        untilDate=str(datetime.utcnow())[:10],\n",
    "                        sID=None, rtmin=2,count=10000):\n",
    "    counter=0\n",
    "    updateC=0\n",
    "    newEntries=0\n",
    "    for status in tweepy.Cursor(api.search, include_rts =False, include_replies=False, \n",
    "                                since=sinceDate, until=untilDate,tweet_mode=\"extended\"\n",
    "                                ,q=query+\" exclude:retweets AND exclude:replies AND min_retweets:\"+str(rtmin), lang=\"en\").items():\n",
    "        # process status here\n",
    "        if(insertToDB(status,stockinfo,query)):updateC+=1            \n",
    "        else:newEntries+=1\n",
    "        counter+=1\n",
    "        if counter>=count:\n",
    "            break\n",
    "    if counter!=count: print(\"End Reached\")\n",
    "    print(stockinfo+\"\\tEntries: {0}\\tUpdated: {1}\\tNew: {2}\".format(counter,updateC,newEntries))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArchiveStore(q, sinceDate, untilDate,stockInfo=\"\", limit=100):\n",
    "    t=datetime.utcnow()\n",
    "    count=0\n",
    "    for tweet in tweepy.Cursor(api.search_full_archive,environment_name ='Sentiment', query =q+\" lang:en\",\n",
    "                                fromDate=sinceDate, toDate=untilDate).items():\n",
    "        tweetText=\"String Error Did Not Find String.\"\n",
    "        sensitive=None\n",
    "        if hasattr(tweet, \"retweeted_status\"):\n",
    "            try:\n",
    "                tweetText=(tweet.retweeted_status.extended_tweet[\"full_text\"])\n",
    "            except AttributeError:\n",
    "                tweetText=(tweet.retweeted_status.text)\n",
    "        else:\n",
    "            try:\n",
    "                tweetText=(tweet.extended_tweet[\"full_text\"])\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    tweetText=(tweet.full_text)\n",
    "                except AttributeError:\n",
    "                    tweetText=(tweet.text)\n",
    "        try:\n",
    "            sensitive=str(tweet.possibly_sensitive)\n",
    "        except: \n",
    "            tweet.possibly_sensitive=\"Unknown\"\n",
    "        count+=1\n",
    "        item= {     \"dateRetrieved\"    :t,\n",
    "                    \"dateCreated\"      :tweet.created_at,\n",
    "                    \"interval(S)\"      :(t-tweet.created_at).total_seconds(),\n",
    "                    \"stockInfo\"        :stockInfo,\n",
    "                    \"query\"            :q,\n",
    "                    \"_id\"              :str(tweet.id_str),\n",
    "                    \"poster\"           :str(tweet.user.screen_name),\n",
    "                    \"posterID\"         :str(tweet.user.id_str),\n",
    "                    \"posterVerified\"   :tweet.user.verified,\n",
    "                    \"followers\"        :int(tweet.user.followers_count),\n",
    "                    \"friends\"          :int(tweet.user.friends_count),\n",
    "                    \"likes\"            :int(tweet.favorite_count),\n",
    "                    \"retweets\"         :int(tweet.retweet_count),\n",
    "                    \"quotes\"           :int(tweet.quote_count),\n",
    "                    \"tweetText\"        :tweetText,\n",
    "                    \"sensitive\"        :sensitive,\n",
    "                    \"isRetweet\"        :tweet.retweeted,\n",
    "                    \"isQuote\"          :tweet.is_quote_status\n",
    "                  }\n",
    "        try:\n",
    "            unprocessedArchive.insert_one(item)\n",
    "        except pymongo.errors.DuplicateKeyError:\n",
    "            print(\"duplicate entry found\")\n",
    "            unprocessedArchive.replace_one({'_id': tweetID}, item, upsert=True)\n",
    "        if(count>=limit):\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStockData(stock,tick=False,save=True):\n",
    "    stocktick = yf.Ticker(stock)\n",
    "    stockhistory=stocktick.history(period=\"max\")\n",
    "    if save: dump(stockhistory, stock)\n",
    "    if tick: return stockhistory,stocktick\n",
    "    return stockhistory\n",
    "\n",
    "def getQueryFromMongoDB(tablename,query={},fields=None):\n",
    "    table=mydb[tablename]\n",
    "    values=[]\n",
    "    if fields is not None:\n",
    "        for entry in table.find(query,fields):\n",
    "            values.append(entry)\n",
    "    else :\n",
    "        for entry in table.find(query):\n",
    "            values.append(entry)\n",
    "    return pd.DataFrame(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizations \n",
    "punctuations=set(string.punctuation)\n",
    "stopwordset=set(stopwords.words('english'))\n",
    "trans=str.maketrans('','',string.punctuation)#removes punctuations\n",
    "#String preprocessing method\n",
    "    \n",
    "#Prediction system pipline\n",
    "def getSentimentNBModel(analyzer=\"word\",ngram=(1,2)):\n",
    "    sentimentModel = Pipeline([\n",
    "        ('bow', CountVectorizer(analyzer=analyzer,ngram_range=ngram)),  # strings to token integer counts\n",
    "        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "        ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "    ])\n",
    "    return sentimentModel\n",
    "def getSentimentLRModel(analyzer=\"word\",ngram=(1,2)):\n",
    "    sentimentModel = Pipeline([\n",
    "        ('bow', CountVectorizer(analyzer=analyzer,ngram_range=ngram)),  # strings to token integer counts\n",
    "        ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "        ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "    ])\n",
    "    return sentimentModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date=\"{0}{1}{2}\".format(dt.year,dt.month,dt.day)+\"0000\"\n",
    "# s=\"cookie\"\n",
    "# date=  \"201701012000\"\n",
    "# todate=\"201701012315\"\n",
    "# ArchiveStore(q=s, sinceDate=date, untilDate=todate,limit =1)\n",
    "# dt=(datetime.utcnow() - timedelta(days=1))\n",
    "# collectDataInterval(\"cookie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file= \"project/a/TrainiingNOemoticon.csv\"\n",
    "# df = pd.read_csv(file, header=None)\n",
    "# for index,row in df.iterrows():\n",
    "#     x,t_Id,date,c,user,text=row\n",
    "#     item= {     \n",
    "#             \"SentimentScore\"   :x,\n",
    "#             \"_id\"              :t_Id,\n",
    "#             \"date\"             :date,\n",
    "#             \"poster\"           :user,\n",
    "#             \"tweetText\"        :text\n",
    "#           }\n",
    "#     try:\n",
    "#         sentiment140.insert_one(item)\n",
    "#     except pymongo.errors.DuplicateKeyError:\n",
    "#         print(\"duplicate entry found\")\n",
    "#         continue\n",
    "# file= \"project/archive/Stocks/tsla.us.txt\"\n",
    "# sentiment140 = mydb[\"stock inf\"]\n",
    "# df = pd.read_csv(file, header=None,encoding='latin-1')\n",
    "# for index,row in df.iterrows():\n",
    "#     x,t_Id,date,c,user,text=row\n",
    "#     item= {     \n",
    "#             \"Date\":,\n",
    "#             \"Open\":,\n",
    "#             \"High\":,\n",
    "#             \"Low\":,\n",
    "#             \"Close\":,\n",
    "#             \"Volume\":,\n",
    "#             \"OpenInt\":\n",
    "#           }\n",
    "#     try:\n",
    "#         sentiment140.insert_one(item)\n",
    "#     except pymongo.errors.DuplicateKeyError:\n",
    "#         print(\"duplicate entry found\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentimentComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMissingInterval(data):\n",
    "    missingIntervals=[]\n",
    "    start=None\n",
    "    end=None\n",
    "    daysBetween=0\n",
    "    ss=pd.date_range(data.index.min(), data.index.max())\n",
    "    for i in range(len(ss)):\n",
    "        if((ss[i].strftime('%Y-%m-%d')==data.index).sum()>=1):\n",
    "            if start!=None: \n",
    "                missingIntervals.append([start,ss[i],daysBetween])\n",
    "                start=None\n",
    "                daysBetween=0\n",
    "        elif((ss[i].strftime('%Y-%m-%d')==data.index).sum()==0):\n",
    "            if(start==None): start=ss[i-1]\n",
    "            daysBetween+=1\n",
    "    return pd.DataFrame(data=missingIntervals,columns=['From', 'To', 'DaysGap'])\n",
    "def checkNonEmptyRuns(data):\n",
    "    runs=[]\n",
    "    start=None\n",
    "    end=None\n",
    "    daysBetween=0\n",
    "    total=0\n",
    "    ss=pd.date_range(data.index.min(), data.index.max())\n",
    "    for i in range(len(ss)):\n",
    "        if((ss[i].strftime('%Y-%m-%d')==data.index).sum()>=1):\n",
    "            if(start==None): \n",
    "                start=ss[i]\n",
    "            daysBetween+=1\n",
    "        elif((ss[i].strftime('%Y-%m-%d')==data.index).sum()==0):\n",
    "            if start!=None: \n",
    "                runs.append([start,ss[i-1],daysBetween])\n",
    "                start=None\n",
    "                total+=daysBetween\n",
    "                daysBetween=0\n",
    "    if start!=None: \n",
    "        runs.append([start,ss[-1],daysBetween])\n",
    "        total+=daysBetween\n",
    "    return pd.DataFrame(data=runs,columns=['From', 'To', 'Days'])\n",
    "def dataSquencer(data,sequenceLength=12,endLength=1):\n",
    "    xSequences=[]\n",
    "    ySequences=[]\n",
    "    sequenceLength+=endLength\n",
    "    entryList=[]\n",
    "    ss=pd.date_range(data.index.min(), data.index.max())\n",
    "    daysBetween=0\n",
    "    for i in range(len(ss)):\n",
    "        sss=ss[i].strftime('%Y-%m-%d')\n",
    "        if((sss==data.index).sum()>=1):\n",
    "            entryList.append(data.loc[sss].values)\n",
    "            daysBetween+=1\n",
    "        elif((sss==data.index).sum()==0):\n",
    "            if(sequenceLength<=daysBetween):\n",
    "                xxs,yys=sequenceBreak(entryList,sequenceLength-endLength,endLength)\n",
    "                xSequences.extend(xxs)\n",
    "                ySequences.extend(yys)\n",
    "            entryList=[]\n",
    "            daysBetween=0\n",
    "    if(sequenceLength<=daysBetween):\n",
    "        xxs,yys=sequenceBreak(entryList,sequenceLength-endLength,endLength)\n",
    "        xSequences.extend(xxs)\n",
    "        ySequences.extend(yys)\n",
    "    return xSequences,ySequences\n",
    "def sequenceBreak(data,sequenceLength=12,endLength=1):\n",
    "    size=len(data)-sequenceLength-endLength+1\n",
    "    brokenSequenceXs,brokenSequenceYs=[None]*size,[None]*size\n",
    "    for i in range(size):\n",
    "        brokenSequenceXs[i]=data[i:i+sequenceLength]\n",
    "        brokenSequenceYs[i]=data[sequenceLength+i:i+sequenceLength+endLength]\n",
    "    return brokenSequenceXs, brokenSequenceYs\n",
    "\n",
    "def plotCompare(x,y,labels=None,size=(20,10)):\n",
    "    if labels is None:\n",
    "        labels=[\"X,Y\"]\n",
    "    plt.plot(x, label=labels[0])\n",
    "    plt.plot(y, label=labels[1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def printResults(y,p,plot=False,size=(20,10)):\n",
    "    MeanDirectionAccuracy=np.average((np.sign(y[1:] - y[:-1]) == np.sign(p[1:] - p[:-1])))\n",
    "    RMSE=mean_squared_error(y, p,squared=False)\n",
    "    print(\"MSE :\",RMSE**2,#Mean Squared Error (MSE)\n",
    "          \"\\nRMSE:\",RMSE,#Root Mean Square Error\n",
    "          \"\\nNMSE:\",mean_squared_error(y, p,squared=False)/(max(y)-min(y)),#Normalise Mean Square Error(/max-min)\n",
    "          \"\\nDS  :\",MeanDirectionAccuracy,#MeanDirectionAccuracy\n",
    "          \"\\nWDS :\",MeanDirectionAccuracy/(max(y)-min(y)),#Weighted Directional Symmetry\n",
    "          \"\\nNs  :\",len(y))#Test Samples\n",
    "    if plot:\n",
    "        plotCompare(testY,predictions,[\"actual\",\"Predictions\"],size)\n",
    "        \n",
    "def makeTrainValidationTestSplit(dataX,dataY=None,splits=[0.6,0.2],randomize=False,reshapeX=None,reshapeY=None,state=0):\n",
    "    if randomize: dataX,dataY=shuffle(dataX,dataY, random_state=state)\n",
    "    if reshapeX is not None: dataX=dataX.reshape(reshapeX)\n",
    "    if reshapeY is not None: dataY=dataY.reshape(reshapeY)\n",
    "    TLim=round ((dataX.shape[0])*splits[0])\n",
    "    VLim=round (dataX.shape[0]*splits[1])\n",
    "    trainX,valX,testX=dataX[:TLim],dataX[TLim:VLim+TLim],dataX[VLim+TLim:]\n",
    "    if dataY is not None:\n",
    "        trainY,valY,testY=dataY[:TLim],dataY[TLim:VLim+TLim],dataY[VLim+TLim:]\n",
    "        return trainX,valX,testX,trainY,valY,testY\n",
    "    return trainX,valX,testX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
